{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成数据函数\n",
    "# mean平均值，cov协方差矩阵，M样本点个数，为偶数\n",
    "# 返回值的形状解释:\n",
    "# data.shape = (3, M)\n",
    "# X.shape = (2, M)\n",
    "# Y.shape = (1, M)\n",
    "# data1.shape = (3, M/2)\n",
    "# data2.shape = (3, M/2)\n",
    "def genData(mean1, cov1, mean2, cov2, M):\n",
    "    # 训练集\n",
    "    X1 = np.random.multivariate_normal(mean1, cov1, int(M / 2)).T\n",
    "    X2 = np.random.multivariate_normal(mean2, cov2, int(M / 2)).T\n",
    "    Y1 = np.zeros_like(X1[0]).reshape((1, int(M / 2)))\n",
    "    Y2 = np.ones_like(X2[0]).reshape((1, int(M / 2)))\n",
    "    X = np.c_[X1, X2]\n",
    "    Y = np.c_[Y1, Y2]\n",
    "\n",
    "    # 测试集\n",
    "    XT1 = np.random.multivariate_normal(mean1, cov1, int(M / 2)).T\n",
    "    XT2 = np.random.multivariate_normal(mean2, cov2, int(M / 2)).T\n",
    "    YT1 = np.zeros_like(XT1[0]).reshape((1, int(M / 2)))\n",
    "    YT2 = np.ones_like(XT2[0]).reshape((1, int(M / 2)))\n",
    "    XT = np.c_[XT1, XT2]\n",
    "    YT = np.c_[YT1, YT2]\n",
    "\n",
    "    # 归一化\n",
    "    theMax = np.max(np.c_[X, XT], axis=1).reshape((2, 1))\n",
    "    theMin = np.min(np.c_[X, XT], axis=1).reshape((2, 1))\n",
    "    X = X - theMin\n",
    "    XT = XT - theMin\n",
    "    X = X / (theMax - theMin)\n",
    "    XT = XT / (theMax - theMin)\n",
    "\n",
    "    # 训练集\n",
    "    data1 = np.r_[X[:, 0:int(M / 2)], Y1]\n",
    "    data2 = np.r_[X[:, int(M / 2):], Y2]\n",
    "    data = np.c_[data1, data2]\n",
    "\n",
    "    # 测试集\n",
    "    dataT1 = np.r_[XT[:, 0:int(M / 2)], YT1]\n",
    "    dataT2 = np.r_[XT[:, int(M / 2):], YT2]\n",
    "    dataT = np.c_[dataT1, dataT2]\n",
    "\n",
    "    print(\"data.shape:\", data.shape)\n",
    "    print(\"X.shape:\", X.shape)\n",
    "    print(\"Y.shape:\", Y.shape)\n",
    "    return data, X, Y, data1, data2, dataT, XT, YT, dataT1, dataT2\n",
    "\n",
    "\n",
    "# 初始化W向量\n",
    "# W.shape = (N, 1)\n",
    "def param(N):\n",
    "    if N < 3 or N % 2 != 1:\n",
    "        print('the invaild N')\n",
    "        return\n",
    "    W = np.ones((N, 1))\n",
    "    return W\n",
    "\n",
    "\n",
    "# 初始化用于计算的X（特征数的不同会有不同的X）\n",
    "# X.shape = (N, M)\n",
    "def initX(X, N, M):\n",
    "    temp = X\n",
    "    n = int((N - 3) / 2)\n",
    "    for i in range(n):\n",
    "        X = np.r_[X, temp**(i + 2)]\n",
    "    X = np.r_[np.ones((1, M)), X]\n",
    "    return X\n",
    "\n",
    "\n",
    "# 多项式函数\n",
    "# W是参数，X是样本，N是特征数与W中元素个数相等\n",
    "# N应为大于等于3的奇数\n",
    "# res.shape = (1, M)\n",
    "def H(W, X):\n",
    "    res = np.dot(W.T, X)\n",
    "    return res\n",
    "\n",
    "\n",
    "# 加正则项的多项式函数\n",
    "def RegH(W, X, lamda, M):\n",
    "    reg = lamda / (2 * M) * np.dot(W.T, W)\n",
    "    res = np.dot(W.T, X) + reg * np.ones((1, M))\n",
    "    return res\n",
    "\n",
    "\n",
    "# sigmod函数\n",
    "# sigmod.shape = (1, M)\n",
    "def G(z):\n",
    "    sigmod = np.exp(-1 * z) + np.ones_like(z)\n",
    "    sigmod = 1 / sigmod\n",
    "    return sigmod\n",
    "\n",
    "\n",
    "# loss函数\n",
    "# type(sum) = <class 'numpy.float64'>\n",
    "def loss(W, X, Y, M):\n",
    "    sigmod = G(H(W, X))\n",
    "    sum1 = np.log(sigmod) * Y\n",
    "    sum2 = np.log(np.ones_like(sigmod) - sigmod) * (np.ones_like(Y) - Y)\n",
    "    temp = sum1 + sum2\n",
    "    sum = np.sum(temp)\n",
    "    sum = -1 * sum\n",
    "    sum = sum / M\n",
    "\n",
    "    return sum\n",
    "\n",
    "\n",
    "# 梯度\n",
    "def grad(X_, Y, sigmod, M):\n",
    "    dz = sigmod - Y\n",
    "    grad = np.dot(X_, dz.T)\n",
    "    grad = grad / M\n",
    "    return grad\n",
    "\n",
    "\n",
    "# 加正则项的梯度\n",
    "def RegGrad(X_, Y, sigmod, M, lamda, W):\n",
    "    dz = sigmod - Y\n",
    "    grad = np.dot(X_, dz.T) + lamda * W\n",
    "    grad = grad / M\n",
    "    return grad\n",
    "\n",
    "\n",
    "#共轭梯度法\n",
    "def conGrad(X, Y, W, N, like):\n",
    "    # 对Y做一个近似以满足共轭梯度法的要求\n",
    "    Y = like * Y + (1 - like) * (np.ones_like(Y) - Y)\n",
    "    Y = -1 * np.log(1 / Y - np.ones_like(Y))\n",
    "\n",
    "    b = np.dot(X, Y.T)\n",
    "    X = np.dot(X, X.T)\n",
    "\n",
    "    b = b.reshape((b.size, 1))\n",
    "    x = W\n",
    "    p = b - np.dot(X.T, W)\n",
    "    r = p\n",
    "\n",
    "    for k in range(N):\n",
    "        if (r == np.zeros_like(r)).all():\n",
    "            break\n",
    "        alpha = ((np.dot(r.T, r)) / np.dot(np.dot(X.T, p).T, p))[0][0]\n",
    "        x = x + alpha * p\n",
    "        temp = r\n",
    "        r = r - alpha * np.dot(X.T, p)\n",
    "        belta = np.dot(r.T, r)[0][0] / np.dot(temp.T, temp)[0][0]\n",
    "        p = r + belta * p\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成数据\n",
    "mean1 = [0.25, 0.25]\n",
    "cov1 = 0.017 * np.eye(2)\n",
    "\n",
    "# 不满足朴素贝叶斯假设\n",
    "cov1[0][1] = cov1[0][0] * (-0.4)\n",
    "cov1[1][0] = cov1[0][1]\n",
    "print(cov1)\n",
    "\n",
    "mean2 = [0.75, 0.75]\n",
    "cov2 = cov1\n",
    "M = 1000\n",
    "data, X, Y, data1, data2, dataT, XT, YT, dataT1, dataT2 = genData(\n",
    "    mean1, cov1, mean2, cov2, M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 画出样本点\n",
    "plt.figure()\n",
    "x1, y1 = data1[0:2, :]\n",
    "x2, y2 = data2[0:2, :]\n",
    "plt.plot(x1, y1, '+', color='red')\n",
    "plt.plot(x2, y2, 'x', color='blue')\n",
    "plt.show()\n",
    "\n",
    "#画出测试集\n",
    "plt.figure()\n",
    "x1, y1 = dataT1[0:2, :]\n",
    "x2, y2 = dataT2[0:2, :]\n",
    "plt.plot(x1, y1, '+', color='red')\n",
    "plt.plot(x2, y2, 'x', color='blue')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init\n",
    "N = 11\n",
    "W0 = param(N)\n",
    "X_ = initX(X, N, M)\n",
    "XT_ = initX(XT, N, M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 不带正则项的梯度下降法逻辑回归"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epoch\n",
    "epoch = 800"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning rate\n",
    "lr = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newLoss = loss(W0, X_, Y, M)\n",
    "oldLoss = newLoss\n",
    "sigmod = G(H(W0, X_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in range(epoch):\n",
    "    W0 = W0 - lr * grad(X_, Y, sigmod, M)\n",
    "    oldLoss = newLoss\n",
    "    newLoss = loss(W0, X_, Y, M)\n",
    "    t = oldLoss - newLoss\n",
    "    if t < 0:\n",
    "        lr /= 1.5\n",
    "    # print('第', item, '次迭代:平方损失函数为', newLoss, '与上次相差', t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 画出结果\n",
    "plt.figure()\n",
    "x1, y1 = data1[0:2, :]\n",
    "x2, y2 = data2[0:2, :]\n",
    "plt.plot(x1, y1, '+', color='red')\n",
    "plt.plot(x2, y2, 'x', color='blue')\n",
    "\n",
    "x3 = np.arange(-2, 2, 0.01)\n",
    "y3 = np.arange(-2, 2, 0.01)\n",
    "x3, y3 = np.meshgrid(x3, y3)\n",
    "z3 = W0[0] * np.ones_like(x3)\n",
    "for i in range(1, N, 2):\n",
    "    temp1 = W0[i] * np.power(x3, (i + 1) / 2)\n",
    "    temp2 = W0[i + 1] * np.power(y3, (i + 1) / 2)\n",
    "    z3 += temp1 + temp2\n",
    "plt.contour(x3, y3, z3, 0)\n",
    "plt.xlim((-0.05, 1.05))\n",
    "plt.ylim((-0.05, 1.05))\n",
    "plt.show()\n",
    "\n",
    "# 测试结果\n",
    "plt.figure()\n",
    "x1, y1 = dataT1[0:2, :]\n",
    "x2, y2 = dataT2[0:2, :]\n",
    "plt.plot(x1, y1, '+', color='red')\n",
    "plt.plot(x2, y2, 'x', color='blue')\n",
    "\n",
    "x3 = np.arange(-2, 2, 0.01)\n",
    "y3 = np.arange(-2, 2, 0.01)\n",
    "x3, y3 = np.meshgrid(x3, y3)\n",
    "z3 = W0[0] * np.ones_like(x3)\n",
    "for i in range(1, N, 2):\n",
    "    temp1 = W0[i] * np.power(x3, (i + 1) / 2)\n",
    "    temp2 = W0[i + 1] * np.power(y3, (i + 1) / 2)\n",
    "    z3 += temp1 + temp2\n",
    "plt.contour(x3, y3, z3, 0)\n",
    "plt.xlim((-0.05, 1.05))\n",
    "plt.ylim((-0.05, 1.05))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = G(H(W0, XT_))\n",
    "\n",
    "# print(res>0.5)\n",
    "# print(res.shape)\n",
    "\n",
    "res[res > 0.5] = 1\n",
    "res[res <= 0.5] = 0\n",
    "\n",
    "temp = res - YT\n",
    "temp = np.abs(temp)\n",
    "sum = np.sum(temp)\n",
    "print(sum)\n",
    "\n",
    "print(\"正确率为：%.2f%%\" % (100 - 100 * sum / res.size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 带正则项的梯度下降法逻辑回归"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = param(N)\n",
    "lamda = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epoch\n",
    "epoch = 800"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning rate\n",
    "lr = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newLoss = loss(W1, X_, Y, M)\n",
    "oldLoss = newLoss\n",
    "sigmod = G(RegH(W1, X_, lamda, M))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in range(epoch):\n",
    "    W1 = W1 - lr * RegGrad(X_, Y, sigmod, M, lamda, W1)\n",
    "    oldLoss = newLoss\n",
    "    newLoss = loss(W1, X_, Y, M)\n",
    "    t = oldLoss - newLoss\n",
    "    if t <= 0:\n",
    "        lr /= 1.5\n",
    "    # print('第', item, '次迭代:平方损失函数为', newLoss, '与上次相差', t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 画出结果\n",
    "plt.figure()\n",
    "x1, y1 = data1[0:2, :]\n",
    "x2, y2 = data2[0:2, :]\n",
    "plt.plot(x1, y1, '+', color='red')\n",
    "plt.plot(x2, y2, 'x', color='blue')\n",
    "\n",
    "x3 = np.arange(-2, 2, 0.01)\n",
    "y3 = np.arange(-2, 2, 0.01)\n",
    "x3, y3 = np.meshgrid(x3, y3)\n",
    "z3 = W1[0] * np.ones_like(x3)\n",
    "for i in range(1, N, 2):\n",
    "    temp1 = W1[i] * np.power(x3, (i + 1) / 2)\n",
    "    temp2 = W1[i + 1] * np.power(y3, (i + 1) / 2)\n",
    "    z3 += temp1 + temp2\n",
    "plt.contour(x3, y3, z3, 0)\n",
    "plt.xlim((-0.05, 1.05))\n",
    "plt.ylim((-0.05, 1.05))\n",
    "plt.show()\n",
    "\n",
    "# 测试结果\n",
    "plt.figure()\n",
    "x1, y1 = dataT1[0:2, :]\n",
    "x2, y2 = dataT2[0:2, :]\n",
    "plt.plot(x1, y1, '+', color='red')\n",
    "plt.plot(x2, y2, 'x', color='blue')\n",
    "\n",
    "x3 = np.arange(-2, 2, 0.01)\n",
    "y3 = np.arange(-2, 2, 0.01)\n",
    "x3, y3 = np.meshgrid(x3, y3)\n",
    "z3 = W1[0] * np.ones_like(x3)\n",
    "for i in range(1, N, 2):\n",
    "    temp1 = W1[i] * np.power(x3, (i + 1) / 2)\n",
    "    temp2 = W1[i + 1] * np.power(y3, (i + 1) / 2)\n",
    "    z3 += temp1 + temp2\n",
    "plt.contour(x3, y3, z3, 0)\n",
    "plt.xlim((-0.05, 1.05))\n",
    "plt.ylim((-0.05, 1.05))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = G(H(W1, XT_))\n",
    "res[res > 0.5] = 1\n",
    "res[res <= 0.5] = 0\n",
    "\n",
    "temp = res - YT\n",
    "temp = np.abs(temp)\n",
    "sum = np.sum(temp)\n",
    "print(sum)\n",
    "print(\"正确率为：%.2f%%\" % (100 - 100 * sum / res.size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 共轭梯度法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init\n",
    "like = 0.999\n",
    "N = 11\n",
    "W2 = param(N)\n",
    "X_ = initX(X, N, M)\n",
    "XT_ = initX(XT, N, M)\n",
    "W2 = conGrad(X_, Y, W2, N, like)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 画出结果\n",
    "plt.figure()\n",
    "x1, y1 = data1[0:2, :]\n",
    "x2, y2 = data2[0:2, :]\n",
    "plt.plot(x1, y1, '+', color='red')\n",
    "plt.plot(x2, y2, 'x', color='blue')\n",
    "\n",
    "x3 = np.arange(-2, 2, 0.01)\n",
    "y3 = np.arange(-2, 2, 0.01)\n",
    "x3, y3 = np.meshgrid(x3, y3)\n",
    "z3 = W2[0] * np.ones_like(x3)\n",
    "for i in range(1, N, 2):\n",
    "    temp1 = W2[i] * np.power(x3, (i + 1) / 2)\n",
    "    temp2 = W2[i + 1] * np.power(y3, (i + 1) / 2)\n",
    "    z3 += temp1 + temp2\n",
    "plt.contour(x3, y3, z3, 0)\n",
    "plt.xlim((-0.05, 1.05))\n",
    "plt.ylim((-0.05, 1.05))\n",
    "plt.show()\n",
    "\n",
    "# 测试结果\n",
    "plt.figure()\n",
    "x1, y1 = dataT1[0:2, :]\n",
    "x2, y2 = dataT2[0:2, :]\n",
    "plt.plot(x1, y1, '+', color='red')\n",
    "plt.plot(x2, y2, 'x', color='blue')\n",
    "\n",
    "x3 = np.arange(-2, 2, 0.01)\n",
    "y3 = np.arange(-2, 2, 0.01)\n",
    "x3, y3 = np.meshgrid(x3, y3)\n",
    "z3 = W2[0] * np.ones_like(x3)\n",
    "for i in range(1, N, 2):\n",
    "    temp1 = W2[i] * np.power(x3, (i + 1) / 2)\n",
    "    temp2 = W2[i + 1] * np.power(y3, (i + 1) / 2)\n",
    "    z3 += temp1 + temp2\n",
    "plt.contour(x3, y3, z3, 0)\n",
    "plt.xlim((-0.05, 1.05))\n",
    "plt.ylim((-0.05, 1.05))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = G(H(W2, XT_))\n",
    "res[res > 0.5] = 1\n",
    "res[res <= 0.5] = 0\n",
    "temp = res - YT\n",
    "temp = np.abs(temp)\n",
    "sum = np.sum(temp)\n",
    "print(sum)\n",
    "print(\"正确率为：%.2f%%\" % (100 - 100 * sum / res.size))"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
